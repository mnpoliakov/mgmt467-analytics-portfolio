{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFGDrMH6oASh"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "GFGDrMH6oASh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZFI8W0joASi"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "FZFI8W0joASi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwpFpFQYoASj"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "vwpFpFQYoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYFZvZKhoASj"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "QYFZvZKhoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd75y1hAoASj"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Fd75y1hAoASj"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Prompt the user to upload their kaggle.json file.\n",
        "# This file contains your Kaggle API credentials.\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist.\n",
        "# This is where Kaggle expects to find the credentials file.\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Save the uploaded file to the correct location.\n",
        "# Using the first uploaded file as we expect only one (kaggle.json).\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "\n",
        "# Set file permissions to 0600 (owner read/write only).\n",
        "# This is crucial for security to prevent other users from accessing your API key.\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "# Verify the Kaggle installation by printing the version.\n",
        "# This confirms the CLI is installed and can access the credentials.\n",
        "!kaggle --version\n",
        "\n",
        "# Done: Kaggle setup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "KjXQxZVJZ-Bc",
        "outputId": "7df736e3-1d3e-4b1e-eb40-42298e80a609"
      },
      "id": "KjXQxZVJZ-Bc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b03096c-d86a-4c4d-8968-3b29e748be99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b03096c-d86a-4c4d-8968-3b29e748be99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Vrae8faFmI"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "Z8Vrae8faFmI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHHGMZsHoASj",
        "outputId": "03631ae3-9560-45bf-9146-6e281fb6fbdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: upbeat-aspect-471118-v8\n",
            "Project: upbeat-aspect-471118-v8 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "upbeat-aspect-471118-v8\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION # Ensure REGION is also exported as an environment variable\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "xHHGMZsHoASj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEIykdfioASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "BEIykdfioASk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Check active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQVtt4cSqUn",
        "outputId": "faec1546-65f7-429c-db9b-fb7268024300"
      },
      "id": "gKQVtt4cSqUn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upbeat-aspect-471118-v8\n",
            "Project: upbeat-aspect-471118-v8 | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOb_eekKoASk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "TOb_eekKoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gf8c7gpoASk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "3Gf8c7gpoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjRY9WSFoASk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "MjRY9WSFoASk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEnJj-7doASk"
      },
      "execution_count": 6,
      "outputs": [],
      "source": [
        "# Query BigQuery's INFORMATION_SCHEMA to check the dataset region\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    schema_name,\n",
        "    location\n",
        "FROM\n",
        "    `{project_id}`.INFORMATION_SCHEMA.SCHEMATA\n",
        "WHERE\n",
        "    schema_name = 'netflix';\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "DEnJj-7doASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thMOKW2ZoASk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "thMOKW2ZoASk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2Vy-teoASk"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "pB2Vy-teoASk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: Check active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8llFhsDHE6j",
        "outputId": "7dcbf4d2-4930-4eaa-9789-6dfcf2265257"
      },
      "id": "-8llFhsDHE6j",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upbeat-aspect-471118-v8\n",
            "Project: upbeat-aspect-471118-v8 | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9kJSE4oASl"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "BD9kJSE4oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRhSvGi4oASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "vRhSvGi4oASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI to /content/data\n",
        "# The -d flag specifies the dataset, and -p specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag overwrites files if they exist\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a neat table\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_U7PESet1j",
        "outputId": "fc1525ff-c46f-41b3-9d9d-9033fa16e710"
      },
      "id": "L8_U7PESet1j",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 826MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFvFr29hoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "hFvFr29hoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "they2Q5foASl"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "they2Q5foASl"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verification step: Assert the number of CSV files and print their names\n",
        "import glob\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "num_csv_files = len(csv_files)\n",
        "\n",
        "# Assert that there are exactly six CSV files\n",
        "assert num_csv_files == 6, f\"Expected 6 CSV files, but found {num_csv_files}\"\n",
        "\n",
        "print(f\"Found {num_csv_files} CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(os.path.basename(csv_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpHjGtNxHRHl",
        "outputId": "37edad6c-de67-45d2-affd-86e24f3254bc"
      },
      "id": "NpHjGtNxHRHl",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 CSV files:\n",
            "movies.csv\n",
            "watch_history.csv\n",
            "search_logs.csv\n",
            "users.csv\n",
            "recommendation_logs.csv\n",
            "reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5PQ-P1yoASl"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "X5PQ-P1yoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IZooOUoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "b_IZooOUoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3xtQmu5oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d233177-82e7-4402-db35-dcd620ce30ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-7adb2cc1 in region: us-central1\n",
            "Creating gs://mgmt467-netflix-7adb2cc1/...\n",
            "\n",
            "Uploading files to gs://mgmt467-netflix-7adb2cc1/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-7adb2cc1/netflix/movies.csv\n",
            "Copying file:///content/data/raw/README.md to gs://mgmt467-netflix-7adb2cc1/netflix/README.md\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-7adb2cc1/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-7adb2cc1/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-7adb2cc1/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-7adb2cc1/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-7adb2cc1/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 21.3MiB/s\n",
            "\n",
            "Successfully created bucket: mgmt467-netflix-7adb2cc1 and uploaded files.\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Consistent Source:** GCS provides a stable and versionable location for your data.\n",
            "- **Scalability:** GCS is highly scalable, handling large datasets easily.\n",
            "- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\n",
            "- **Cost-Effective:** GCS can be a cost-effective storage solution.\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-7adb2cc1/netflix/:\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/README.md\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/movies.csv\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/users.csv\n",
            "gs://mgmt467-netflix-7adb2cc1/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Create a unique bucket in the specified region\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Ensure REGION environment variable is set before using it in gcloud command\n",
        "REGION = os.environ.get('REGION')\n",
        "print(f\"Creating bucket: gs://{bucket_name} in region: {REGION}\")\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSVs to the bucket\n",
        "print(f\"\\nUploading files to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name and explain staging benefits\n",
        "print(f\"\\nSuccessfully created bucket: {bucket_name} and uploaded files.\")\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- **Consistent Source:** GCS provides a stable and versionable location for your data.\")\n",
        "print(\"- **Scalability:** GCS is highly scalable, handling large datasets easily.\")\n",
        "print(\"- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\")\n",
        "print(\"- **Cost-Effective:** GCS can be a cost-effective storage solution.\")\n",
        "\n",
        "# Verify contents (optional but recommended)\n",
        "print(f\"\\nVerifying contents of gs://{bucket_name}/netflix/:\")\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "O3xtQmu5oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZhif_knoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "kZhif_knoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwgInaDFoASl"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "CwgInaDFoASl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification step: List objects in the netflix/ prefix with sizes\n",
        "import os\n",
        "\n",
        "bucket_name = os.environ[\"BUCKET_NAME\"]\n",
        "print(f\"Listing contents of gs://{bucket_name}/netflix/ with sizes:\")\n",
        "!gcloud storage ls -l gs://$BUCKET_NAME/netflix/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnBUiPjUHsvh",
        "outputId": "a837b0d4-91bd-4c81-8f4f-e8c60e249192"
      },
      "id": "AnBUiPjUHsvh",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of gs://mgmt467-netflix-7adb2cc1/netflix/ with sizes:\n",
            "      8002  2025-10-27T00:42:51Z  gs://mgmt467-netflix-7adb2cc1/netflix/README.md\n",
            "    115942  2025-10-27T00:42:51Z  gs://mgmt467-netflix-7adb2cc1/netflix/movies.csv\n",
            "   4695557  2025-10-27T00:42:52Z  gs://mgmt467-netflix-7adb2cc1/netflix/recommendation_logs.csv\n",
            "   1861942  2025-10-27T00:42:52Z  gs://mgmt467-netflix-7adb2cc1/netflix/reviews.csv\n",
            "   2250902  2025-10-27T00:42:52Z  gs://mgmt467-netflix-7adb2cc1/netflix/search_logs.csv\n",
            "   1606820  2025-10-27T00:42:52Z  gs://mgmt467-netflix-7adb2cc1/netflix/users.csv\n",
            "   9269425  2025-10-27T00:42:52Z  gs://mgmt467-netflix-7adb2cc1/netflix/watch_history.csv\n",
            "TOTAL: 7 objects, 19808590 bytes (18.89MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ApPnyBoASl"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "k7ApPnyBoASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mvCSfFhoASl"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "3mvCSfFhoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKjSOx8JoASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52d51f0-a28c-48bc-a30d-9fd8a98a5bbc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'upbeat-aspect-471118-v8:netflix'\n",
            "already exists.\n",
            "Dataset may already exist.\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=us-central1 mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "RKjSOx8JoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7e6986",
        "outputId": "8f810476-3037-4143-c332-cf763a0c8231"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the BUCKET_NAME environment variable to the correct bucket name\n",
        "os.environ[\"BUCKET_NAME\"] = \"mgmt467-netflix-81f821b1\"\n",
        "print(f\"BUCKET_NAME environment variable set to: {os.environ['BUCKET_NAME']}\")"
      ],
      "id": "6c7e6986",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BUCKET_NAME environment variable set to: mgmt467-netflix-81f821b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bDr5GH1oASl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4868001f-5226-463f-f584-c2badab7acf7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bucket: mgmt467-netflix-7adb2cc1 for loading data.\n",
            "Loading users from gs://mgmt467-netflix-7adb2cc1/netflix/users.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "Loading movies from gs://mgmt467-netflix-7adb2cc1/netflix/movies.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "Loading watch_history from gs://mgmt467-netflix-7adb2cc1/netflix/watch_history.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "Loading recommendation_logs from gs://mgmt467-netflix-7adb2cc1/netflix/recommendation_logs.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "Loading search_logs from gs://mgmt467-netflix-7adb2cc1/netflix/search_logs.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "Loading reviews from gs://mgmt467-netflix-7adb2cc1/netflix/reviews.csv\n",
            "BigQuery error in load operation: Not found: Dataset upbeat-\n",
            "aspect-471118-v8:upbeat-aspect-471118-v8.netflix\n",
            "\n",
            "Checking row counts:\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.users: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r1516ef5fb7054657_0000019a23253a85_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.movies: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r42526d69e1a20a96_0000019a2325463a_1': Syntax error:\n",
            "Unexpected end of script at [1:50]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.watch_history: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r156e09b68422283_0000019a232553ad_1': Syntax error:\n",
            "Unexpected end of script at [1:57]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.recommendation_logs: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r65cfaf1c5f9b166_0000019a23255f45_1': Syntax error:\n",
            "Unexpected end of script at [1:63]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.search_logs: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r8498d79fc8c04fb_0000019a23256acf_1': Syntax error:\n",
            "Unexpected end of script at [1:55]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.reviews: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r2c97df05d1598282_0000019a232575d3_1': Syntax error:\n",
            "Unexpected end of script at [1:51]\n"
          ]
        }
      ],
      "source": [
        "# EXAMPLE (from LLM) — Load tables (commented)\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET = \"netflix\" # Define DATASET variable\n",
        "PROJECT_ID = os.environ['GOOGLE_CLOUD_PROJECT'] # Define PROJECT_ID variable\n",
        "BUCKET_NAME = os.environ['BUCKET_NAME'] # Get bucket name from environment variable\n",
        "print(f\"Using bucket: {BUCKET_NAME} for loading data.\") # Print bucket name being used\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{BUCKET_NAME}/netflix/{fname}\"\n",
        "  table_ref = f\"{PROJECT_ID}.{DATASET}.{tbl}\" # Construct table reference separately\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Corrected bq load command syntax - use the constructed table_ref\n",
        "  !bq load --source_format=CSV --skip_leading_rows=1 --autodetect {table_ref} {src}\n",
        "\n",
        "# Row counts\n",
        "print(\"\\nChecking row counts:\")\n",
        "for tbl in tables.keys():\n",
        "  # Corrected bq query command syntax\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{PROJECT_ID}.{DATASET}.{tbl}`\""
      ],
      "id": "3bDr5GH1oASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0553045f",
        "outputId": "737cd0ba-ef24-4dbc-bc05-3d10753d4918"
      },
      "source": [
        "# Corrected code to load tables and check row counts\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET = \"netflix\" # Define DATASET variable\n",
        "PROJECT_ID = os.environ['GOOGLE_CLOUD_PROJECT'] # Define PROJECT_ID variable\n",
        "BUCKET_NAME = os.environ['BUCKET_NAME'] # Get bucket name from environment variable\n",
        "print(f\"Using bucket: {BUCKET_NAME} for loading data.\") # Print bucket name being used\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{BUCKET_NAME}/netflix/{fname}\"\n",
        "  table_ref = f\"{PROJECT_ID}.{DATASET}.{tbl}\" # Construct table reference\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Corrected bq load command syntax - removed the unnecessary project_id repetition\n",
        "  !bq load --source_format=CSV --skip_leading_rows=1 --autodetect {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "print(\"\\nChecking row counts:\")\n",
        "for tbl in tables.keys():\n",
        "  # Corrected bq query command syntax\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{PROJECT_ID}.{DATASET}.{tbl}`\""
      ],
      "id": "0553045f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bucket: mgmt467-netflix-81f821b1 for loading data.\n",
            "Loading users from gs://mgmt467-netflix-81f821b1/netflix/users.csv\n",
            "Waiting on bqjob_r4017af867f17b332_00000199ef17e240_1 ... (2s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-81f821b1/netflix/movies.csv\n",
            "Waiting on bqjob_r3e36c2c40f16acef_00000199ef17fbd6_1 ... (2s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-81f821b1/netflix/watch_history.csv\n",
            "Waiting on bqjob_r3bbafe6896ff9589_00000199ef18170b_1 ... (3s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-81f821b1/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r7be3c11949482e48_00000199ef183441_1 ... (2s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-81f821b1/netflix/search_logs.csv\n",
            "Waiting on bqjob_r42d18225bde97bf7_00000199ef184d75_1 ... (2s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-81f821b1/netflix/reviews.csv\n",
            "Waiting on bqjob_r4d0cf9ebea8ef1ba_00000199ef1865cb_1 ... (2s) Current status: DONE   \n",
            "\n",
            "Checking row counts:\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.users: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r65fdc4112d8e7498_00000199ef187da8_1': Syntax error:\n",
            "Unexpected end of script at [1:49]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.movies: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r17f829865cbd45c4_00000199ef188763_1': Syntax error:\n",
            "Unexpected end of script at [1:50]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.watch_history: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r92fae50ad2fc967_00000199ef1890c0_1': Syntax error:\n",
            "Unexpected end of script at [1:57]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.recommendation_logs: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r1943f539b4453bb7_00000199ef189a51_1': Syntax error:\n",
            "Unexpected end of script at [1:63]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.search_logs: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r32146b72e98b3a07_00000199ef18a6cd_1': Syntax error:\n",
            "Unexpected end of script at [1:55]\n",
            "/bin/bash: line 1: upbeat-aspect-471118-v8.netflix.reviews: command not found\n",
            "Error in query string: Error processing job 'upbeat-\n",
            "aspect-471118-v8:bqjob_r5952e354f9616cce_00000199ef18b04a_1': Syntax error:\n",
            "Unexpected end of script at [1:51]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "# Hardcoded dataset ID\n",
        "DATASET_ID = \"upbeat-aspect-471118-v8.netflix\"\n",
        "# Hardcoded bucket name from previous successful run - REPLACE WITH YOUR ACTUAL BUCKET NAME if needed\n",
        "BUCKET_NAME = \"mgmt467-netflix-7adb2cc1\"\n",
        "\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  uri = f\"gs://{BUCKET_NAME}/netflix/{fname}\"\n",
        "  table_id = f\"{DATASET_ID}.{tbl}\"\n",
        "\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "      autodetect=True,\n",
        "      skip_leading_rows=1,\n",
        "      source_format=bigquery.SourceFormat.CSV,\n",
        "  )\n",
        "\n",
        "  print(f\"Loading {tbl} from {uri} into {table_id}\")\n",
        "  # Use the client library to load the data\n",
        "  load_job = client.load_table_from_uri(\n",
        "      uri, table_id, job_config=job_config\n",
        "  )  # Make an API request.\n",
        "\n",
        "  load_job.result()  # Waits for the job to complete.\n",
        "\n",
        "  print(f\"Load job for {tbl} completed.\")\n",
        "\n",
        "# Row counts (Verification Prompt)\n",
        "print(\"\\nVerifying row counts:\")\n",
        "for tbl in tables.keys():\n",
        "    table_id_full = f\"{DATASET_ID}.{tbl}\"\n",
        "    query = f\"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{table_id_full}`\"\n",
        "    query_job = client.query(query)\n",
        "    results = query_job.result()\n",
        "    for row in results:\n",
        "        print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty6MTMhEJn_n",
        "outputId": "b28243e8-7f00-4920-e273-13819f0c8f60"
      },
      "id": "Ty6MTMhEJn_n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-7adb2cc1/netflix/users.csv into upbeat-aspect-471118-v8.netflix.users\n",
            "Load job for users completed.\n",
            "Loading movies from gs://mgmt467-netflix-7adb2cc1/netflix/movies.csv into upbeat-aspect-471118-v8.netflix.movies\n",
            "Load job for movies completed.\n",
            "Loading watch_history from gs://mgmt467-netflix-7adb2cc1/netflix/watch_history.csv into upbeat-aspect-471118-v8.netflix.watch_history\n",
            "Load job for watch_history completed.\n",
            "Loading recommendation_logs from gs://mgmt467-netflix-7adb2cc1/netflix/recommendation_logs.csv into upbeat-aspect-471118-v8.netflix.recommendation_logs\n",
            "Load job for recommendation_logs completed.\n",
            "Loading search_logs from gs://mgmt467-netflix-7adb2cc1/netflix/search_logs.csv into upbeat-aspect-471118-v8.netflix.search_logs\n",
            "Load job for search_logs completed.\n",
            "Loading reviews from gs://mgmt467-netflix-7adb2cc1/netflix/reviews.csv into upbeat-aspect-471118-v8.netflix.reviews\n",
            "Load job for reviews completed.\n",
            "\n",
            "Verifying row counts:\n",
            "Row(('users', 30900), {'table_name': 0, 'n': 1})\n",
            "Row(('movies', 3120), {'table_name': 0, 'n': 1})\n",
            "Row(('watch_history', 315000), {'table_name': 0, 'n': 1})\n",
            "Row(('recommendation_logs', 156000), {'table_name': 0, 'n': 1})\n",
            "Row(('search_logs', 79500), {'table_name': 0, 'n': 1})\n",
            "Row(('reviews', 46350), {'table_name': 0, 'n': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEU0_sBoASl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "yqEU0_sBoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e948d81",
        "outputId": "9843f289-1e38-4666-e308-9397371b9da0"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT 'users' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.reviews`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "9e948d81",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('recommendation_logs', 156000), {'table_name': 0, 'row_count': 1})\n",
            "Row(('users', 30900), {'table_name': 0, 'row_count': 1})\n",
            "Row(('search_logs', 79500), {'table_name': 0, 'row_count': 1})\n",
            "Row(('movies', 3120), {'table_name': 0, 'row_count': 1})\n",
            "Row(('reviews', 46350), {'table_name': 0, 'row_count': 1})\n",
            "Row(('watch_history', 315000), {'table_name': 0, 'row_count': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeM4Ik7moASl"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "xeM4Ik7moASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOxNJk_oASl"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "3TOxNJk_oASl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nht9dQqOoASl"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "nht9dQqOoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d35d84",
        "outputId": "a9f7ff3a-8f2c-4c5c-ba24-5e1ed30ef64a"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Verification: Print missingness percentages\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "d4d35d84",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0.0, 0.0, 11.93), {'pct_missing_country': 0, 'pct_missing_subscription_plan': 1, 'pct_missing_age': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D2oFpiSoASl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "4D2oFpiSoASl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca7662d6",
        "outputId": "b21a4878-cb68-422d-805a-d2f1a7103789"
      },
      "source": [
        "# Total rows and % missing in country, subscription_plan, age from users\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_rows,\n",
        "  ROUND(100 * COUNTIF(country IS NULL) / COUNT(*), 2) AS pct_missing_country,\n",
        "  ROUND(100 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * COUNTIF(age IS NULL) / COUNT(*), 2) AS pct_missing_age\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "ca7662d6",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((30900, 0.0, 0.0, 11.93), {'total_rows': 0, 'pct_missing_country': 1, 'pct_missing_subscription_plan': 2, 'pct_missing_age': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fff023f",
        "outputId": "8c232852-aa3a-449f-84c4-98289bc61987"
      },
      "source": [
        "# % subscription_plan missing by country ordered descending (Checking for MAR - Missing At Random)\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  country,\n",
        "  COUNT(*) AS n,\n",
        "  ROUND(100 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan\n",
        "FROM `{project_id}.netflix.users`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_subscription_plan DESC;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "2fff023f",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('USA', 21612, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n",
            "Row(('Canada', 9288, 0.0), {'country': 0, 'n': 1, 'pct_missing_subscription_plan': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflection: Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "metadata": {
        "id": "pUgD_9cgLKcS"
      },
      "id": "pUgD_9cgLKcS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of the missingness analysis:\n",
        "\n",
        "The age column has the highest percentage of missing values (11.93%). country and subscription_plan have no missing values.\n",
        "\n",
        "Regarding hypotheses for the missingness:\n",
        "\n",
        "Age: It's possible the missingness in age is MAR (Missing At Random). The reason for not providing age might be related to other demographic information or user behavior that is captured in the dataset. For example, users in a certain age range might be less likely to provide their age. It could also be MNAR (Missing Not At Random) if there's a systemic reason why certain users (e.g., very young or very old users) are less likely to have their age recorded, and this reason is not captured by other variables.\n",
        "Country and Subscription Plan: Since there are no missing values for these columns, we don't need to hypothesize about MCAR/MAR/MNAR. The data for these columns appears to be complete.\n",
        "We would need to perform further analysis to confirm if the missingness is indeed MAR or MNAR, for example, by examining the characteristics of users with missing age values compared to those with complete age data.\n",
        "\n"
      ],
      "metadata": {
        "id": "j79H9-gGLi2Q"
      },
      "id": "j79H9-gGLi2Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "metadata": {
        "id": "HWf6t0QYMXxJ"
      },
      "id": "HWf6t0QYMXxJ"
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Report duplicate groups on (user_id, movie_id, watch_date, device_type) with counts (top 20)\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDDw7lFuMjj6",
        "outputId": "31cc9ed8-99b9-4637-d26c-212d7b113115"
      },
      "id": "PDDw7lFuMjj6",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 12), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03140', 'movie_0205', datetime.date(2025, 9, 11), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08157', 'movie_0729', datetime.date(2025, 10, 26), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00928', 'movie_0913', datetime.date(2024, 1, 18), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07529', 'movie_0686', datetime.date(2025, 7, 7), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00249', 'movie_0203', datetime.date(2024, 8, 31), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04657', 'movie_0857', datetime.date(2025, 3, 4), 'Mobile', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06799', 'movie_0458', datetime.date(2024, 8, 15), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06535', 'movie_0890', datetime.date(2025, 5, 23), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06295', 'movie_0097', datetime.date(2025, 2, 24), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06901', 'movie_0478', datetime.date(2025, 12, 10), 'Mobile', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08799', 'movie_0427', datetime.date(2024, 1, 27), 'Smart TV', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03043', 'movie_0465', datetime.date(2024, 2, 3), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_04506', 'movie_0244', datetime.date(2025, 5, 27), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00794', 'movie_0852', datetime.date(2024, 2, 9), 'Smart TV', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01581', 'movie_0933', datetime.date(2024, 3, 30), 'Desktop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00564', 'movie_0234', datetime.date(2024, 1, 9), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01807', 'movie_0921', datetime.date(2025, 1, 30), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_09815', 'movie_0827', datetime.date(2024, 5, 25), 'Laptop', 9), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Create table watch_history_dedup keeping one row per group\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "query_job.result()\n",
        "\n",
        "print(f\"Table `{project_id}.netflix.watch_history_dedup` created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8cxiDMdMmTi",
        "outputId": "67b937c2-9045-414c-e283-f48a4dbf5606"
      },
      "id": "y8cxiDMdMmTi",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table `upbeat-aspect-471118-v8.netflix.watch_history_dedup` created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Verification: Before/after count query comparing raw vs watch_history_dedup\n",
        "SELECT 'watch_history_raw' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'watch_history_dedup' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history_dedup`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcJfnYI6NBlz",
        "outputId": "84b29089-3c56-49bb-cf87-42eddb908e1a"
      },
      "id": "RcJfnYI6NBlz",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('watch_history_raw', 315000), {'table_name': 0, 'row_count': 1})\n",
            "Row(('watch_history_dedup', 100000), {'table_name': 0, 'row_count': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "metadata": {
        "id": "eRSKWRU9NSP3"
      },
      "id": "eRSKWRU9NSP3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicates can arise from various sources, both natural and system-generated.\n",
        "\n",
        "Natural Duplicates: These occur when real-world events are recorded multiple times. For example, a user might refresh a page, leading to the same watch event being logged twice in quick succession.\n",
        "System-Generated Duplicates: These are often due to errors in data pipelines, ETL processes, or application logic. Retries, misconfigurations, or bugs can cause records to be inserted or processed more than once.\n",
        "Duplicates can significantly corrupt labels and KPIs:\n",
        "\n",
        "Inflated Metrics: Simple counts (like number of views or sessions) will be artificially high, leading to an overestimation of activity.\n",
        "Incorrect Aggregations: Averages, sums, and other aggregations will be skewed. For instance, average watch duration could be underestimated if partial watch events are duplicated.\n",
        "Biased Model Training: If duplicate data is used to train machine learning models, the models can become biased towards the characteristics of the duplicated records. This can lead to poor performance on clean data.\n",
        "Skewed Analysis: Any analysis based on the duplicated data will be inaccurate, leading to flawed business decisions.\n",
        "Handling duplicates is crucial for ensuring data integrity and the reliability of downstream analysis and models."
      ],
      "metadata": {
        "id": "l1CJ40iENaMl"
      },
      "id": "l1CJ40iENaMl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "metadata": {
        "id": "-1SoV8ZVONaV"
      },
      "id": "-1SoV8ZVONaV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after."
      ],
      "metadata": {
        "id": "cX21lBHPONkC"
      },
      "id": "cX21lBHPONkC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db206a8",
        "outputId": "87fb87a2-946f-4254-91f8-572fc66126a1"
      },
      "source": [
        "# Compute IQR bounds for watch_duration_minutes on watch_history_dedup and report % outliers\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "4db206a8",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((3433, 100000, 3.43), {'outliers': 0, 'total': 1, 'pct_outliers': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e77e7159",
        "outputId": "fd64d609-1642-48e4-bbc0-0ee2649208b8"
      },
      "source": [
        "# Create watch_history_robust with watch_duration_minutes_capped at P01/P99; return quantile summaries before/after.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "e77e7159",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('before', [0.2, 24.9, 41.7, 61.4, 91.5, 799.3]), {'which': 0, 'q': 1})\n",
            "Row(('after', [4.4, 24.6, 41.5, 61.5, 92.0, 203.6]), {'which': 0, 'q': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping."
      ],
      "metadata": {
        "id": "bXdifpuoOfUW"
      },
      "id": "bXdifpuoOfUW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54d328bb",
        "outputId": "dd0d628b-fc71-474f-c312-7fa595fa3f4a"
      },
      "source": [
        "# Verification: Show min/median/max before vs after capping\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH before AS (\n",
        "  SELECT\n",
        "    'before' AS which,\n",
        "    MIN(watch_duration_minutes) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT\n",
        "    'after' AS which,\n",
        "    MIN(watch_duration_minutes_capped) AS min_val,\n",
        "    APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_val,\n",
        "    MAX(watch_duration_minutes_capped) AS max_val\n",
        "  FROM `{project_id}.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "54d328bb",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('after', 4.4, 51.4, 203.6), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n",
            "Row(('before', 0.2, 51.2, 799.3), {'which': 0, 'min_val': 1, 'median_val': 2, 'max_val': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "metadata": {
        "id": "WHxV0Ox3Or8e"
      },
      "id": "WHxV0Ox3Or8e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering capping outliers, it's important to think about when it might not be the best approach:\n",
        "\n",
        "Loss of Information: If the outliers are genuine data points that hold important information (e.g., a customer with exceptionally high spending), capping them can lead to a loss of valuable insights.\n",
        "Distorted Relationships: Capping can distort the relationship between variables, which might negatively impact models that rely on these relationships.\n",
        "Misleading Results: If the goal is to understand the full range of the data or identify extreme cases, capping can mask these important aspects.\n",
        "Model types that are generally less sensitive to outliers include tree-based models such as Decision Trees, Random Forests, and Gradient Boosting Machines (like XGBoost or LightGBM). These models partition the data based on feature values and are not influenced by the magnitude of extreme values in the same way that linear models or distance-based algorithms are. They are more focused on the rank order of data points and splits."
      ],
      "metadata": {
        "id": "H8eedK-MOwip"
      },
      "id": "H8eedK-MOwip"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "metadata": {
        "id": "HugaUwSbPDYw"
      },
      "id": "HugaUwSbPDYw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments."
      ],
      "metadata": {
        "id": "JEpi502QPDg1"
      },
      "id": "JEpi502QPDg1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff2b209c",
        "outputId": "4e4de4b4-cbda-4d5c-dada-fbf8deac8e8a"
      },
      "source": [
        "# In watch_history_robust, compute and summarize flag_binge for sessions > 8 hours.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes_capped > 8*60)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "ff2b209c",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((0, 100000, 0.0), {'sessions_over_8h': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45f74499",
        "outputId": "4d495028-3632-48ab-85d9-55ddcf0e08f9"
      },
      "source": [
        "# In users, compute and summarize flag_age_extreme if age is <10 or >100.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `{project_id}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "45f74499",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((537, 30900, 1.74), {'extreme_age_rows': 0, 'total': 1, 'pct': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524e9007",
        "outputId": "9ec8872e-de15-485e-a279-8194ebea2fc1"
      },
      "source": [
        "# In movies, compute and summarize flag_duration_anomaly where duration_minutes < 15 or > 480.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "524e9007",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row((36, 33, 3120, 2.21), {'titles_under_15m': 0, 'titles_over_8h': 1, 'total': 2, 'pct_duration_anomaly': 3})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c9573e0",
        "outputId": "a2a4f590-b1a8-4117-92e6-d01a3b7b226b"
      },
      "source": [
        "# Verification: Single compact summary query for anomaly flags\n",
        "import os\n",
        "# from google.cloud import bigquery # Comment out the import\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "location = 'us-central1' # Specify location here\n",
        "\n",
        "# client = bigquery.Client(project=project_id, location=location) # Comment out client initialization\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  'flag_binge' AS flag_name,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes > 8*60)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_age_extreme' AS flag_name,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT\n",
        "  'flag_duration_anomaly' AS flag_name,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `{project_id}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "# Use bq query command-line tool\n",
        "#!bq query --nouse_legacy_sql --location={location} \"{query}\"\n",
        "\n",
        "# The results from bq query are printed directly to stdout, no need for fetching results with client.\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# # Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "8c9573e0",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('flag_binge', 0.64), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_age_extreme', 1.74), {'flag_name': 0, 'pct_of_rows': 1})\n",
            "Row(('flag_duration_anomaly', 2.21), {'flag_name': 0, 'pct_of_rows': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "metadata": {
        "id": "He6VcH3yQ66p"
      },
      "id": "He6VcH3yQ66p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the anomaly flag analysis:\n",
        "\n",
        "The flag_duration_anomaly is the most common, with 2.21% of movies having a duration less than 15 minutes or greater than 480 minutes.\n",
        "\n",
        "As for which flag to keep as a feature, it depends on the specific downstream task. However, flag_binge (sessions > 8 hours) could be a valuable feature for several reasons:\n",
        "\n",
        "Predictive Power: Binge-watching behavior is often a strong indicator of user engagement and could be highly predictive for models related to user retention, churn, or content recommendations.\n",
        "Business Relevance: Identifying binge sessions is directly relevant to understanding user consumption patterns and can inform content strategy and platform design.\n",
        "Actionable Insight: This flag provides a clear signal about a specific type of user behavior that can be targeted for analysis or interventions.\n",
        "While flag_age_extreme and flag_duration_anomaly also identify interesting patterns, flag_binge seems to capture a more direct and potentially impactful user behavior for many common Netflix-related modeling tasks."
      ],
      "metadata": {
        "id": "XTIfFpu1Q8eI"
      },
      "id": "XTIfFpu1Q8eI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKDRvHtoASr"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "8yKDRvHtoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMy5HEutoASr"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "PMy5HEutoASr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3qqUtyoASr"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "Iy3qqUtyoASr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd4e12a0",
        "outputId": "acb6e944-ce08-443b-997f-ac1acb081de2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fd4e12a0",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff113628",
        "outputId": "1eba0421-a00d-4486-c6b7-0050d084b31a"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access the notebook file\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'YOUR_NOTEBOOK_PATH_IN_DRIVE' with the actual path to your notebook file in Google Drive\n",
        "# For example: '/content/drive/My Drive/Colab Notebooks/your_notebook_name.ipynb'\n",
        "notebook_path = '/content/drive/MyDrive/MGMT467/Labs/Lab4_MGMT467_PromptPlusExamples_Colab_Kaggle_GCS_BQ_DQ_prof.ipynb' # <--- **MODIFY THIS LINE**\n",
        "\n",
        "try:\n",
        "    # Read the notebook as a JSON file\n",
        "    with open(notebook_path, 'r') as f:\n",
        "        notebook_content = json.load(f)\n",
        "\n",
        "    sql_queries = []\n",
        "\n",
        "    # Regular expression to find the SQL query string within the f-string\n",
        "    # It looks for `query = f\"\"\"` followed by any characters (non-greedily),\n",
        "    # and captures the content before the closing `\"\"\"`.\n",
        "    # It also handles the case where the f-string might span multiple lines.\n",
        "    sql_pattern = re.compile(r'query\\s*=\\s*f\"\"\"(.*?)\"\"\"', re.DOTALL)\n",
        "\n",
        "    # Iterate through cells and extract SQL queries from code cells\n",
        "    for cell in notebook_content.get('cells', []):\n",
        "        if cell.get('cell_type') == 'code':\n",
        "            # Join the list of strings into a single string\n",
        "            source_code = \"\".join(cell.get('source', []))\n",
        "            match = sql_pattern.search(source_code)\n",
        "            if match:\n",
        "                # Extract the captured group (the SQL query string)\n",
        "                query = match.group(1).strip()\n",
        "                sql_queries.append(query)\n",
        "\n",
        "    # Write the extracted SQL queries to a .sql file\n",
        "    output_filename = 'dq_queries.sql'\n",
        "    with open(output_filename, 'w') as f:\n",
        "        for i, query in enumerate(sql_queries):\n",
        "            f.write(f\"-- Query {i+1}\\n\")\n",
        "            f.write(query)\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "    print(f\"SQL queries exported to {output_filename}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Notebook file not found at {notebook_path}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode the notebook file as JSON. Ensure it's a valid .ipynb file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "id": "ff113628",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL queries exported to dq_queries.sql\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}